---
title: "Sentiment Analysis of Amazon Food Reviews"
author: "Likhita Pula"
date: "7/28/2021"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits = 1)
require(dplyr)
require(ggplot2)
require(RColorBrewer)
require(rpart)
require(tidyverse)
require(Rgraphviz)
require(RSQLite)
```

## Sentiment Analysis of Amazon Food Reviews

## Project Statement

The key objective of this project is to analyze the sentiment of customers that are reviewing food products on amazon.com

This project is divided into two parts:

In Part 1 lexicon-based approach is used to gather insights on different types of sentiments experienced by customers

In Part 2 using score as target value a supervised machine learning model is built that can predict customers sentiment based on textual review.


```{r}

conn <- dbConnect(SQLite(), "C:\\Users\\likhi\\Desktop\\MSDA\\Data Mining 2\\Dataset\\database.sqlite")

data_amazon <- dbGetQuery(conn, "SELECT * FROM Reviews")

head(data_amazon)

```

We can now check the data type, dimension and other important information about the extracted data.

```{r}
str(data_amazon)
```
We can see that there are about 10 Columns or attributes and 568454 rows/ records in our dataset.

The amazon food review data has below attributes:
* Id
* Product Id - unique identifier for the product
* User Id - unique identifier for the user
* Profile Name
* Helpfulness Numerator - number of users who found the review helpful
* Helpfulness Denominator - number of users who indicated whether they found the review helpful or not
* Score - rating between 1 and 5
* Time - time stamp for the review
* Summary - brief summary of the review
* Text - text of the review

## Exploratory Data Analysis

As a first step we check if there are any empty of NUll values in the "Text" column of the dataset

```{r}
dbGetQuery(conn, "SELECT Text
                  FROM Reviews
                  WHERE Text IS NULL")

```

Since there are no null values in the "Text" column, each row can be treated as a document to create corpus. However, we can further explore the dataset to find any abnormalities

```{r}
distinct_products <- dbGetQuery(conn, "SELECT DISTINCT ProductId
                  FROM Reviews ")
dim(distinct_products)
```

For my analysis I would like to check only popular products which can be identified as products that are receiving at least greater than 3 reviews. Thus I checked the products that have high number of reviews. 

```{r}
popular_products <- dbGetQuery(conn, "SELECT ProductId
                  FROM Reviews
                  GROUP BY ProductId
                  HAVING COUNT(Text)> 3
                  ORDER BY COUNT(Text) DESC")

dim(popular_products)
head(popular_products)
```

Checking the reviews on the product "B007JFMH8M".

```{r}
most_popluar_product <- dbGetQuery(conn, "SELECT Text
                  FROM Reviews
                  WHERE ProductId = 'B007JFMH8M'")

dim(most_popluar_product)

```

There are about 913 reviews on this product which clearly indicates that a lot of people have used this product. We can assume that the higher review rates indicate that the product is selling more and maybe liked by customers. But we can't say this certainty that the customers have positive sentiment towards this product as high reviews can also mean people are negatively reviewing so that others don't buy the product.


Hence we can further explore the sentiment of customers towards all the popular products (i.e. The products having number of reviews at least greater than 3 reviews). Also I just want to analyze positive and negative sentiment reviews thus I am removing the reviews with neutral score of 3


```{r}
data_retrieved <- dbGetQuery(conn,"SELECT *
          		      FROM Reviews
          		      WHERE Score != 3 
          		      GROUP BY ProductId
                    HAVING COUNT(Text)> 3")
dim(data_retrieved)
```


```{r}
dbDisconnect(conn)
```

```{r}
duplicate_check1 <- filter(data_retrieved, Id == 171104)
duplicate_check2 <- filter(data_retrieved, Id == 217335)
duplicate_check1$Text
duplicate_check2$Text
```

While exploring the data, we can see that there are two reviews made by same user as same time. This can be a case of duplicate record for products that are of same type but different size.

It would not be useful to analyze same textual review that have been given at as point of time by the same user/customer as these can be duplicate records or there is a chance that the organization applies same review to products that are same but different in size. so I have tried to remove duplicates of textual reviews from the dataset based on the UserId , Time and Text columns.

```{r}
data <-  data_retrieved[!duplicated(data_retrieved[c("UserId","Time","Text")]),] 
dim(data)
```

```{r}
head(data)
```

Converting the UNIX time to real date format

```{r}
data$Time <- lubridate::as_datetime(data$Time)
data$Year <- lubridate::year(data$Time)
```

```{r}
unique(data$Year)
```

As, I have already removed the neutral rating score of 3 from my dataset it would be good add a new attribute into the data which gives out true (1) for Score<3 or false (0) for Score>3 as at the end of my analysis I would validate the positive and negative sentiment generated from text reviews.

```{r}
data$Negative_Review <- as.factor(data$Score < 3)

head(data[1:4,12])

```


## Data Visualization

First, we would like to examine the customers who have reviewed most of the food products from the selected data of popular products. Any customer who has given 25 or above reviews on food products over the years is considered a frequent reviewer.

```{r}
user_count <-  data %>% 
                     count(UserId)
par(mar=c(9, 9, 2, 1))
user_count <- user_count[order(-user_count$n),]
x <- user_count[1:5,]
cols <- brewer.pal(5, "Set2") 
barplot(height=x$n, names=x$UserId, col=cols, las = 2, main="Top 5 Frequent Reviewers")

```
```{r}
x$UserId

```

As these are the customers who have given most of the reviews, it would be interesting to see the distribution of Score (rating) that they have provided to different products

```{r}

top_user_review <- data[data$UserId %in% c("A1YUL9PCJR3JTY","A3OXHLG6DIBRW8", "A281NPSIMI1C2R","A1LZJZIHUPLDV4","A1WX42M589VAMQ"),]

top_user_score <-  top_user_review %>% 
                     group_by(UserId) %>% 
                       count(Score)
                 
top_user_score$UserId <- factor(top_user_score$UserId)
top_user_score$Score <- factor(top_user_score$Score)     

ggplot(data=top_user_score, aes(x=" ", y=n, group=Score, colour=Score, fill=Score)) +
         geom_bar(width = 1, stat = "identity") +
         coord_polar("y", start=0) + 
         facet_grid(.~ UserId) +theme_void()
         
```


Seeing the above plot we can say that frequent reviewers are usually providing good score rating (5 & 4) and there are hardly any negative ratings provided by these customers. Using this information company can encourage these frequent reviewers to reviewer more items which would ultimately enhance the brand value.

Since we have data from 2002 to 2012, which is about 10 years of data. It would be interesting to see how many products are getting more than 3 reviews per each year.

```{r}
review_year <- data %>%
  select(Text, Year) %>%
  group_by(Year) %>%
  summarise(review_count = n())
plot(review_year$Year, review_year$review_count,type = "b", pch = 19, col = "blue",
     main="Total Reviews Received on popular products over year")
```

We can see that there has been an incremental growth in the number products receiving greater than 3 reviews each year however there is a slight decrease from 2011 to 2012 which is indicating that there might be increase in number of reviews but a slight decrease in popular product reviews.

It would be good to see if this increase is only in the positive reviews or if its in both negative as well as positive reviews.

```{r}
review_neg <- data %>%
      group_by(Year) %>%
      count(Negative_Review)
pos_check1 <- as.data.frame(filter(review_neg, Negative_Review == "FALSE"))
neg_check2 <- as.data.frame(filter(review_neg, Negative_Review == "TRUE"))

neg_check2 <- neg_check2 %>% add_row(Year = 2002, n = 0)
neg_check2 <- neg_check2[order(neg_check2$Year),]
review_year <- as.data.frame(review_year)
review_year$Negative_review <- neg_check2$n
review_year$Positive_review <- pos_check1$n
reshape_data <- reshape2::melt(review_year,id.vars="Year")
barchart_data <- reshape_data[12:33,]
```

```{r}
ggplot(barchart_data) +  geom_bar(aes(x=Year,y=value,fill=variable), stat="identity",position="dodge")+
  ggtitle("Total Reviews Received on Popular Products Over Years") 
```
We can see that over the years there has been increase in positive as well as negative corresponding to the total number of reviews on popular products.

## Data Preprocessing

Since I want to use the text data to check the sentiment of customers reviews, as a first step corpus is created for text data

Convert all letters to lowercase
```{r}

data$Text <- stringr::str_to_lower(data$Text)

```

Remove special character strings such as websites and email

```{r}
data$Text <- qdapRegex::rm_url(
  data$Text,
  replacement = " ",
  clean = TRUE
)

data$Text <- qdapRegex::rm_hash(
  data$Text,
  replacement = " ",
  clean = TRUE
)
data$Text <- qdapRegex::rm_tag(
  data$Text,
  replacement = " ",
  clean = TRUE
)
data$Text <- qdapRegex::rm_emoticon(
  data$Text,
  replacement = " ",
  clean = TRUE
)
data$Text <- qdapRegex::rm_email(
  data$Text,
  replacement = " ",
  clean = TRUE
)

```

Remove stop words

```{r}
data$Text <- tm::removeWords(
  x = data$Text,
  words = tm::stopwords(kind = "SMART")
)
data$Text <- tm::removeWords(
  x = data$Text,
  words = tm::stopwords(kind = "english")
)
data$Text <- tm::removeWords(
  x = data$Text,
  words = qdapDictionaries::Top200Words
)


```

Get rid of extra white space.

```{r}
data$Text <- trimws(stringr::str_replace_all(
  string = data$Text,
  pattern = "\\s+",
  replacement = " "
))

```

Removing Punctuation and Numbers

```{r}
data$Text <- tm::removePunctuation(
  x = data$Text
) 

data$Text <- tm::removeNumbers(
  x = data$Text
) 

```

Create Corpus

```{r}

Corpus_reviews <- iconv(data$Text)
Corpus_reviews <- tm::VCorpus(tm::VectorSource(data$Text))
tm::inspect(Corpus_reviews[[15]])

```

Removing White space from corpus

```{r}
Corpus_reviews <- tm::tm_map(Corpus_reviews, tm::stripWhitespace)
```

Creating Document Term Matrix of the review corpus

```{r}

DocumentTermMatrix_reviews <- tm::DocumentTermMatrix(Corpus_reviews)

```

Remove sparse terms.

```{r}

DocumentTermMatrix_reviews <- tm::removeSparseTerms(
  DocumentTermMatrix_reviews,
  0.995
)

```

Create a integer matrix equivalent to the term document matrix

```{r}

M <- as.matrix(DocumentTermMatrix_reviews)
M[1:5,1:5]
dim(M)
```

```{r}

term_frequency <- data.frame(
  Term = colnames(M),
  Frequency = colSums(M),
  stringsAsFactors = FALSE
)
term_frequency <- term_frequency[order(term_frequency$Frequency),]

term_frequency
```


```{r}

wordcloud::wordcloud(
  words = term_frequency$Term,
  freq = term_frequency$Frequency,
  max.words = 25,
  random.order = FALSE,
  colors = viridis::viridis(100)
)


```

From the word cloud, we can see that there are few words like "amazon" or "product" which are not very informative as this data set is all about amazon food reviews.

Removing some uninformative words

```{r}
Corpus_reviews <- tm::tm_map(Corpus_reviews,tm::removeWords, c("amazon", "order","buy","food","product","bought","add","knowing","common"))

```

Now, visualizing the word cloud

```{r}
DocumentTermMatrix_reviews <- tm::DocumentTermMatrix(Corpus_reviews)
DocumentTermMatrix_reviews <- tm::removeSparseTerms(
  DocumentTermMatrix_reviews,
  0.995
)
M <- as.matrix(DocumentTermMatrix_reviews)
term_frequency <- data.frame(
  Term = colnames(M),
  Frequency = colSums(M),
  stringsAsFactors = FALSE
)
term_frequency <- term_frequency[order(term_frequency$Frequency),]

wordcloud::wordcloud(
  words = term_frequency$Term,
  freq = term_frequency$Frequency,
  max.words = 25,
  random.order = FALSE,
  colors = viridis::viridis(100)
)

```

Looking at the word cloud we can make the below deductions:

* looks like tea is a popular product which is greatly reviewed by people. "Coffee" is also appearing in that word cloud which intuitively tells us that people are reviewing beverages more compared to other food products.
* Also there are few key words like "delicious" & "love" indicates that overall people reviewing these products have a more of positive sentiment.
* The use of words like "price", "taste" and "flavor" intuitively tells that the value of a product and products taste as well as flavor influences customer's sentiment. It looks like "taste" and "flavor" has greater influence on the customers review than the "price" of product.

```{r}
dim(term_frequency)
```

```{r}

barplot(term_frequency[1174:1170,]$Frequency, las = 2, names.arg = term_frequency[1174:1170,]$Term,
        col ="purple", main ="Top 5 most frequent words",
        ylab = "Word frequencies")

```

Seeing the above barplot we can say that beverages like tea and coffee are most reviewed products as the frequency of these terms is high. Also words like "love" appear more in the reviews indicating that most of the reviews would be positive.

**Building a bayesian network for Top 10 frequent words**

```{r}
term_frequency[1174:1165,]$Term
```

```{r}
Network <- as.data.frame(M)
Bn <- Network[,c("tea", "taste", "flavor", "coffee","love" ,"price","sugar",    "chocolate" ,"eat", "bag"  )]
```

```{r}

model_amazon <- bnlearn::hc(Bn)
bnlearn::graphviz.plot(model_amazon,
                       shape = "ellipse")

```
This bayesian network is complex but one key deduction we can make from the network is that the conditional probability of the word "tea" appearing in the documents is dependent on the words "love", "taste" and "flavor" appearing in the document. This means that the product tea is associated with a positive sentiment word of love.

## Using syuzhet Lexicon for sentiment analysis

```{r}
sentiment <- syuzhet::get_nrc_sentiment(data$Text)
head(sentiment)

```


```{r}

sentiment_M <- as.matrix(sentiment)
barplot(sentiment_M, border = "dark blue",las = 2)
```

From the above plot showing frequency of each sentiment we can say that most of the reviews are positive followed by trust and joy. There are less negative sentiment in customer reviews when compared to the positive reviews. Also the prevalence of anger, disgust, fear, sadness and surprise sentiments which is a good sign for overall foods that are sold on amazon website.


## Building a model for sentiment prediction

Since we have already built the sparse document term matrix we can now split the text data into training set and testing set which would be used to train our model for sentiment prediction.

```{r}
review_df <- as.data.frame(M)
colnames(review_df) <- make.names(colnames(review_df))
review_df$Negative_reviews <- data$Negative_Review

set.seed(813)
split <- caTools::sample.split(review_df$Negative_reviews, SplitRatio=0.7)
train_reviews <- subset(review_df, split == TRUE)
test_reviews <- subset(review_df, split == FALSE)
```


Considering a baseline model that predicts all predictions as positive,

```{r}
table(test_reviews$Negative_reviews)
```

Then the accuracy of out baseline model is 5377/(5377+711) = 0.883.


First, I am building a Recursive Partitioning and Regression Trees (rpart) model to predict the positive and negative sentiments

```{r}
review_model <- rpart::rpart(Negative_reviews ~ ., data = train_reviews, method ="class")
predict_m <- predict(review_model, newdata = test_reviews, type="class")

```

```{r}
cm <- caret::confusionMatrix(test_reviews$Negative_reviews,predict_m)
fourfoldplot(cm$table)
```
```{r}
cm
```
The accuracy of the ecursive Partitioning and Regression Trees (rpart) model is about 0.885 which is indicating that is model has a slignt improvement on accurately predicting positive and negative sentiments compared to baseline.

Next, I am building a Support Vector Machine model to make sentiment predictions

```{r}
svm_model <- e1071::svm(Negative_reviews ~ ., data = train_reviews)
predict_svm <- predict(svm_model, test_reviews)
```

```{r}
cm2 <- caret::confusionMatrix(test_reviews$Negative_reviews,predict_svm)
fourfoldplot(cm2$table)
```

```{r}
cm2 
```

The accuracy of the Support Vector Machine model is about 0.904 which is better than the initial Recursive Partitioning and Regression Trees model that was used

Thus, it would be good to use Support Vector Machine model for predicting positive & negative sentiment of customers based on their text reviews.

## Conclusion


Through the analysis done in this project we are able to analyze the sentiments of customers and gain some insights based on their reviews that can help the organization in future strategy and roadmap development. Also we are able to predict the positive and negative sentiment of customer based on their textual reviews using a model that has accuracy of about 0.9.
